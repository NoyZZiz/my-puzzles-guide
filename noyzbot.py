import os
import random
import pinecone
from dotenv import load_dotenv
from pinecone import Pinecone
from transformers import AutoTokenizer, AutoModel
import torch
from flask import Flask, request, jsonify
from flask_cors import CORS
from sentence_transformers import SentenceTransformer

# âœ… Load environment variables
load_dotenv()
app = Flask(__name__)
CORS(app)  # âœ… Allow all origins locally for testing

# âœ… Initialize Pinecone client
pc = Pinecone(api_key=os.getenv("PINECONE_API_KEY"))

# âœ… Get Pinecone Index Name
index_name = os.getenv("PINECONE_INDEX_NAME")

# âœ… Connect to the existing Pinecone index
if index_name not in pc.list_indexes().names():
    print(f"âŒ Index '{index_name}' not found. Ensure Pinecone is properly set up.")
else:
    index = pc.Index(index_name)
    print(f"âœ… Connected to Pinecone index: {index_name}")

# âœ… Load Hugging Face Embedding Model
tokenizer = AutoTokenizer.from_pretrained("BAAI/bge-large-en")
model = AutoModel.from_pretrained("BAAI/bge-large-en", torch_dtype="auto", device_map="auto")
intent_model = SentenceTransformer("all-MiniLM-L6-v2")  # âœ… Intent detection model

# âœ… Function to convert user queries into embeddings
def get_embedding(text):
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
    outputs = model(**inputs)
    embedding = outputs.last_hidden_state.mean(dim=1).detach().numpy().tolist()[0]

    # âœ… Debugging Step: Print Embedding Size
    print(f"ğŸ” Query Embedding Shape: {len(embedding)}")
    print(f"ğŸ” Query Embedding Values (First 10): {embedding[:10]}")

    return embedding

# âœ… Function to detect intent
def detect_intent(text):
    keywords = {
        "strategy": ["strategy", "troop", "battle", "attack", "defense"],
        "guides": ["guide", "help", "how to", "tips", "advice"],
        "calculator": ["calculate", "troop calculator", "resource calculator"],
    }
    for intent, words in keywords.items():
        if any(word in text.lower() for word in words):
            return intent
    return "general"

# âœ… List of Random Sassy Intros
sassy_intros = [
    "Oh, you again? What do you need now? ğŸ˜",
    "Welcome! Iâ€™m just a bot training like you. Donâ€™t expect too much. ğŸ˜‰",
    "You want answers? I got 'em. But donâ€™t expect sugarcoating. ğŸ”¥",
    "Another day, another question. Let's see if you can impress me. ğŸ˜",
    "I donâ€™t do free coaching, but since youâ€™re hereâ€¦ ask away. ğŸ¤–",
    "If I donâ€™t know the answer, itâ€™s because your question is bad. ğŸ˜‚"
]

def get_random_intro():
    """Returns a random sassy intro message."""
    return random.choice(sassy_intros)

@app.route("/", methods=["GET"])
def home():
    return "NoyzBot is running locally!"

# âœ… Flask App Setup
@app.route("/chat", methods=["POST"])
def chat():
    user_input = request.json.get("user_input", "").lower()
    print(f"ğŸ“¥ Received Message from User: {user_input}")

    # âœ… Step 1: Check Custom Responses First
    custom_responses = {
        "who created this website": "Noyzzing created this platform to help players master Puzzles and Conquest!",
        "who made this guide": "This guide was made by Noyzzing with contributions from Lisette and the community.",
        "what is siege troop": "Siege troops? Really? They are *useless*. You should never train them. ğŸš¨"
    }

    if user_input in custom_responses:
        print(f"ğŸ¯ Matched Custom Response: {custom_responses[user_input]}")
        return jsonify({"response": custom_responses[user_input]})

    # âœ… Step 2: Detect Intent
    intent = detect_intent(user_input)
    print(f"ğŸ” Detected Intent: {intent}")

    # âœ… Step 3: Handle Broad Questions (Ask for Clarification)
    if intent == "general":
        return jsonify({"response": "Hmm... can you be more specific? Are you asking about troops, battles, or guides?"})

    # âœ… Step 4: Convert to Embedding & Query Pinecone
    query_vector = get_embedding(user_input)
    print(f"ğŸ” Query Embedding Shape: {len(query_vector)}")

    search_results = index.query(vector=query_vector, top_k=1, include_metadata=True)
    print(f"ğŸ” Search Results: {search_results}")

    if search_results and "matches" in search_results and search_results["matches"]:
        best_match = search_results["matches"][0]
        metadata = best_match["metadata"]
        
        # âœ… Fix: Check if "text" exists, otherwise check for "answer"
        response = metadata.get("text") or metadata.get("answer") or "I couldn't find an answer for that!"
        
        category = metadata.get("category", "general")

        # âœ… Fix: Only roast Siege users if they are asking about Siege
        if category == "siege" and "siege" in user_input.lower():
            print(f"ğŸ”¥ Roasting Siege Users: {response}")
            return jsonify({"response": response})

        print(f"âœ… Returning Response: {response}")
        return jsonify({"response": response})

    # âœ… Step 5: Fallback Response
    print("âŒ No match found, returning fallback.")
    return jsonify({"response": "I don't have that answer yet, but I'm learning! Try something else."})
# âœ… Run Flask Locally
if __name__ == "__main__":
    print("ğŸš€ Starting Flask server...")
    app.run(host="127.0.0.1", port=5000, debug=True)  # âœ… Run locally at http://127.0.0.1:5000